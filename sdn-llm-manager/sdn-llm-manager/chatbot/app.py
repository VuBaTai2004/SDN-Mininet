import gradio as gr
import sys
import os
from datetime import datetime

# ThÃªm thÆ° má»¥c cha vÃ o path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.ollama_client import OllamaClient
from llm.prompt_templates import get_full_prompt
from chatbot.command_parser import CommandParser


class SDNChatbot:
    """
    Chatbot interface cho SDN management
    """
    
    def __init__(self):
        print("Khá»Ÿi táº¡o SDN Chatbot...")
        
        # Khá»Ÿi táº¡o LLM client
        self.llm = OllamaClient(model="llama3.2")
        
        # Khá»Ÿi táº¡o command parser
        self.parser = CommandParser()
        
        # Lá»‹ch sá»­ chat
        self.chat_history = []
        
        print("âœ“ Chatbot Ä‘Ã£ sáºµn sÃ ng!")
    
    def process_message(self, message, history):
        """
        Xá»­ lÃ½ message tá»« user
        
        Args:
            message: Message tá»« user
            history: Lá»‹ch sá»­ chat (format Gradio)
        
        Returns:
            str: Response tá»« bot
        """
        try:
            # Log user message
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"\n[{timestamp}] User: {message}")
            
            # Táº¡o prompt cho LLM
            full_prompt = get_full_prompt(message)
            
            # Gá»i LLM
            print(f"[{timestamp}] Äang xá»­ lÃ½ vá»›i LLM...")
            llm_response = self.llm.generate(full_prompt, temperature=0.3)
            
            if not llm_response:
                return "âŒ Lá»—i: KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n LLM. Kiá»ƒm tra xem Ollama cÃ³ Ä‘ang cháº¡y khÃ´ng (cháº¡y: ollama serve)"
            
            print(f"[{timestamp}] LLM response: {llm_response[:200]}...")
            
            # Parse command tá»« LLM response
            parse_result = self.parser.parse(llm_response)
            
            if parse_result['status'] == 'error':
                return f"âŒ {parse_result['message']}\n\nğŸ’¬ Response tá»« LLM:\n{llm_response}"
            
            command = parse_result['command']
            explanation = command.get('explanation', '')
            
            # Thá»±c thi command
            print(f"[{timestamp}] Äang thá»±c thi: {command['action']}")
            exec_result = self.parser.execute(command)
            
            # Format response
            response = self._format_response(command, exec_result, explanation)
            
            print(f"[{timestamp}] âœ“ HoÃ n thÃ nh")
            return response
            
        except Exception as e:
            print(f"âŒ Lá»—i: {str(e)}")
            return f"âŒ ÄÃ£ xáº£y ra lá»—i: {str(e)}"
    
    def _format_response(self, command, exec_result, explanation):
        """
        Format response Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘áº¹p
        """
        response_parts = []
        
        # Explanation tá»« LLM
        if explanation:
            response_parts.append(f"ğŸ’¡ **{explanation}**\n")
        
        # Káº¿t quáº£ thá»±c thi
        if exec_result['status'] == 'success':
            response_parts.append(f"âœ… {exec_result.get('message', 'ThÃ nh cÃ´ng')}")
            
            # Data náº¿u cÃ³
            if 'data' in exec_result:
                data = exec_result['data']
                
                # Format topology data
                if command['action'] == 'show_topology':
                    response_parts.append(f"\nğŸ“Š **ThÃ´ng tin máº¡ng:**")
                    response_parts.append(f"- Sá»‘ switches: {data['num_switches']}")
                    response_parts.append(f"- Sá»‘ links: {data['num_links']}")
                    
                    if data['switches']:
                        response_parts.append(f"\n**Danh sÃ¡ch switches:**")
                        for dpid, info in data['switches'].items():
                            response_parts.append(f"  - Switch {dpid}: {info.get('ports', 0)} ports")
                    
                    if data['links']:
                        response_parts.append(f"\n**Danh sÃ¡ch links:**")
                        for link in data['links'][:5]:  # Chá»‰ show 5 links Ä‘áº§u
                            response_parts.append(
                                f"  - Switch {link['src']} port {link['src_port']} "
                                f"â†” Switch {link['dst']} port {link['dst_port']}"
                            )
                
                # Format flows data
                elif command['action'] == 'show_flows':
                    response_parts.append(f"\nğŸ“‹ **Flow Rules ({len(data)} rules):**")
                    for i, flow in enumerate(data[:10], 1):  # Show 10 flows Ä‘áº§u
                        response_parts.append(
                            f"\n{i}. Switch {flow.get('switch_id')} | "
                            f"Priority: {flow.get('priority')} | "
                            f"Match: {flow.get('match', 'N/A')[:50]}"
                        )
                    if len(data) > 10:
                        response_parts.append(f"\n... vÃ  {len(data) - 10} flows khÃ¡c")
        
        else:
            response_parts.append(f"âŒ {exec_result.get('message', 'CÃ³ lá»—i xáº£y ra')}")
            
            # Note náº¿u cÃ³
            if 'note' in exec_result:
                response_parts.append(f"\nâ„¹ï¸ {exec_result['note']}")
        
        return "\n".join(response_parts)


def create_interface():
    """
    Táº¡o Gradio interface
    """
    chatbot_instance = SDNChatbot()
    
    # Custom CSS
    custom_css = """
    .gradio-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    .message-wrap {
        font-size: 14px;
    }
    """
    
    # Táº¡o interface
    with gr.Blocks(css=custom_css, title="SDN LLM Manager") as demo:
        gr.Markdown("""
        # ğŸŒ SDN LLM Manager
        ### Quáº£n lÃ½ máº¡ng SDN thÃ´ng qua Natural Language
        
        Báº¡n cÃ³ thá»ƒ:
        - ğŸ“Š Xem topology máº¡ng
        - â• ThÃªm/xÃ³a flow rules
        - ğŸš« Cháº·n/cho phÃ©p traffic
        - âš™ï¸ Cáº¥u hÃ¬nh QoS
        
        **VÃ­ dá»¥:**
        - "Cho tÃ´i xem cáº¥u trÃºc máº¡ng"
        - "ThÃªm rule forward tá»« 10.0.0.1 Ä‘áº¿n 10.0.0.2"
        - "Cháº·n traffic tá»« 192.168.1.100"
        """)
        
        chatbot = gr.Chatbot(
            height=500,
            show_label=False,
            avatar_images=[None, "ğŸ¤–"]
        )
        
        with gr.Row():
            msg = gr.Textbox(
                placeholder="Nháº­p yÃªu cáº§u cá»§a báº¡n...",
                show_label=False,
                scale=9
            )
            submit = gr.Button("Gá»­i", scale=1, variant="primary")
        
        gr.Examples(
            examples=[
                "Xem topology máº¡ng",
                "Hiá»ƒn thá»‹ táº¥t cáº£ flow rules",
                "ThÃªm flow tá»« h1 Ä‘áº¿n h2 qua switch 1",
                "Cháº·n traffic tá»« 10.0.0.5",
                "Äáº·t bÄƒng thÃ´ng 5Mbps tá»« 10.0.0.1 Ä‘áº¿n 10.0.0.2"
            ],
            inputs=msg,
            label="ğŸ“ CÃ¢u lá»‡nh máº«u"
        )
        
        gr.Markdown("""
        ---
        **LÆ°u Ã½:**
        - Äáº£m báº£o Ryu Controller Ä‘ang cháº¡y: `ryu-manager controller/ryu_controller.py`
        - Äáº£m báº£o Mininet Ä‘ang cháº¡y: `sudo python3 mininet/topology.py`
        - Äáº£m báº£o Ollama Ä‘ang cháº¡y: `ollama serve`
        """)
        
        # Xá»­ lÃ½ submit
        def respond(message, chat_history):
            if not message.strip():
                return "", chat_history
            
            # ThÃªm message vÃ o history
            chat_history.append([message, None])
            
            # Xá»­ lÃ½ vá»›i chatbot
            bot_response = chatbot_instance.process_message(message, chat_history)
            
            # Update history
            chat_history[-1][1] = bot_response
            
            return "", chat_history
        
        submit.click(respond, [msg, chatbot], [msg, chatbot])
        msg.submit(respond, [msg, chatbot], [msg, chatbot])
    
    return demo


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     SDN LLM Manager - Chatbot UI        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Kiá»ƒm tra dependencies
    print("Kiá»ƒm tra mÃ´i trÆ°á»ng...")
    
    try:
        import requests
        # Test Ollama
        try:
            resp = requests.get("http://localhost:11434/api/tags", timeout=2)
            print("âœ“ Ollama Ä‘ang cháº¡y")
        except:
            print("âš  Cáº£nh bÃ¡o: Ollama chÆ°a cháº¡y. Khá»Ÿi Ä‘á»™ng báº±ng: ollama serve")
        
        # Test Ryu Controller
        try:
            resp = requests.get("http://localhost:8080/sdn/topology", timeout=2)
            print("âœ“ Ryu Controller Ä‘ang cháº¡y")
        except:
            print("âš  Cáº£nh bÃ¡o: Ryu Controller chÆ°a cháº¡y")
    except:
        pass
    
    print("\nKhá»Ÿi Ä‘á»™ng Chatbot UI...")
    
    # Táº¡o vÃ  cháº¡y interface
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
